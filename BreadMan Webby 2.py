import requests
from bs4 import BeautifulSoup
import re
import os
import tkinter as tk
from tkinter import messagebox, filedialog
from PIL import ImageTk, Image 
import html


# Define a list of common HTML vulnerabilities to detect
vulnerabilities = {
    'script': {'regex': re.compile(r'<script>', re.IGNORECASE), 'severity': 'high'},
    'onload': {'regex': re.compile(r'onload', re.IGNORECASE), 'severity': 'medium'},
    'iframe': {'regex': re.compile(r'<iframe>', re.IGNORECASE), 'severity': 'high'},
    'meta': {'regex': re.compile(r'<meta>', re.IGNORECASE), 'severity': 'low'},
    'javascript': {'regex': re.compile(r'javascript:', re.IGNORECASE), 'severity': 'high'},
    'data': {'regex': re.compile(r'data:', re.IGNORECASE), 'severity': 'high'}
}

# Define a list of HTTP headers to check
headers_to_check = ['X-Frame-Options', 'X-XSS-Protection', 'Content-Security-Policy', 'X-Content-Type-Options']

# Define a list of allowed file extensions
allowed_extensions = ['.txt', '.pdf', '.doc', '.docx']

# Create the GUI window
window = tk.Tk()
window.title("SourDough Web Crawler")
window.geometry("800x800")

# Set background image
bg_image = ImageTk.PhotoImage(Image.open("BreadMan.png"))
background_label = tk.Label(window, image=bg_image)
background_label.place(x=0, y=0, relwidth=1, relheight=1)

# Add header label
header_label = tk.Label(window, text="SourDough Web Vulnerability Scanner", font=("Fixedsys", 20))
header_label.pack(pady=20)

# Add URL label and entry
url_label = tk.Label(window, text="Bread Bag to Scan:", font=("Fixedsys", 18))
url_label.pack(pady=10)

url_entry = tk.Entry(window, width=40)
url_entry.pack()

# Add Start and Stop buttons
def start_scan():
    url = url_entry.get()
    if url:
        scan_website(url)
    else:
        messagebox.showerror("Error", "Please enter a URL to scan.")

start_button = tk.Button(window, text="Start", font=("Fixedsys"), width=10, command=start_scan)
start_button.pack(pady=10)

def stop_scan():
    window.destroy()

stop_button = tk.Button(window, text="Stop", font=("Fixedsys"), width=10, command=stop_scan)
stop_button.pack(pady=10)

# Function to perform the website scan
def scan_website(url):
    global results
    results = []

    # Start crawling the website
    urls_to_crawl = [url]
    while urls_to_crawl:
        url = urls_to_crawl.pop()
        response = requests.get(url)  # Enable cert val
        html_content = response.content
        soup = BeautifulSoup(html_content, 'html.parser')  # Specify the HTML parser explicitly

        # Check for common HTML vulnerabilities
        for vulnerability, data in vulnerabilities.items():
            if re.search(data['regex'], html.unescape(str(soup))):
                results.append({'URL': url, 'Vulnerability': vulnerability, 'Severity': data['severity']})

        # Check for common HTTP header vulnerabilities
        headers = response.headers
        for header in headers_to_check:
            if header in headers:
                results.append({'URL': url, 'Vulnerability': header, 'Severity': 'high'})

        # Check for input field vulnerabilities
        forms = soup.find_all('form')
        for form in forms:
            inputs = form.find_all('input')
            for input in inputs:
                input_type = input.get('type')
                input_name = input.get('name')
                if input_type == 'file' and input_name:
                    file_field = form.find('input', {'name': input_name, 'type': 'file'})
                    if file_field and 'value' in file_field.attrs:
                        file_name = file_field['value']
                        if os.path.splitext(file_name)[1].lower() not in allowed_extensions:
                            results.append({'URL': url, 'Vulnerability': f"Invalid file extension: {file_name}", 'Severity': 'high'})

        # Add the links on the page to the list of URLs to crawl
        for link in soup.find_all('a', href=True):  # Only select links with a valid href attribute
            link_url = link['href']
            if link_url.startswith(url) and link_url not in urls_to_crawl:
                urls_to_crawl.append(link_url)

    # Save the results to a file
    file_path = filedialog.asksaveasfilename(defaultextension=".txt", filetypes=[("Text Files", "*.txt")])
    if file_path:
        with open(file_path, 'w') as f:
            for result in results:
                f.write(f"URL: {result['URL']}\nVulnerability: {result['Vulnerability']}\nSeverity: {result['Severity']}\n\n")
        messagebox.showinfo("Scan Completed", "Vulnerability scan completed successfully.\nResults saved to file.")

# Start the GUI event loop
window.mainloop()
